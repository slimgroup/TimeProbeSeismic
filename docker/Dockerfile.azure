FROM python:3.8

ENV DEBIAN_FRONTEND noninteractive 

# Install nvidia HPC sdk
RUN apt-get update -y && \
    apt-get install -y -q wget git apt-utils vim curl && \
    apt-get install -y -q liblapack-dev libblas-dev libibverbs-dev && \
    wget -q -P /app/ https://developer.download.nvidia.com/hpc-sdk/21.5/nvhpc-21-5_21.5_amd64.deb \ 
         https://developer.download.nvidia.com/hpc-sdk/21.5/nvhpc-2021_21.5_amd64.deb && \
    apt-get install -y -q /app/nvhpc-21-5_21.5_amd64.deb /app/nvhpc-2021_21.5_amd64.deb && \
    apt-get update -y && \
    rm -rf /app/nvhpc* && \
    rm -rf /var/lib/apt/lists/*

ARG HPCSDK_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/2021
ARG HPCSDK_CUPTI=/opt/nvidia/hpc_sdk/Linux_x86_64/2021/cuda/11.3/extras/CUPTI

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

# required for nvidia-docker v1
RUN echo "$HPCSDK_HOME/cuda/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/cuda/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/compilers/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/comm_libs/mpi/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_CUPTI/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/math_libs/lib64" >> /etc/ld.so.conf.d/nvidia.conf  

# Compiler, CUDA, and Library paths
ENV CUDA_HOME $HPCSDK_HOME/cuda
ENV CUDA_ROOT $HPCSDK_HOME/cuda/bin
ENV PATH $HPCSDK_HOME/compilers/bin:$HPCSDK_HOME/cuda/bin:$HPCSDK_HOME/comm_libs/mpi/bin:${PATH}
ENV LD_LIBRARY_PATH $HPCSDK_HOME/cuda/lib:$HPCSDK_HOME/cuda/lib64:$HPCSDK_HOME/compilers/lib:$HPCSDK_HOME/math_libs/lib64:$HPCSDK_HOME/comm_libs/mpi/lib:$HPCSDK_CUPTI/lib64:${LD_LIBRARY_PATH}

# Make julia use preinstalled cuda
ENV JULIA_CUDA_USE_BINARYBUILDER false

# Install Julia
RUN wget "https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.1-linux-x86_64.tar.gz" && \
    tar -xvzf julia-1.6.1-linux-x86_64.tar.gz && \
    rm -rf julia-1.6.1-linux-x86_64.tar.gz && \
    ln -s /julia-1.6.1/bin/julia /usr/local/bin/julia

# Install requirements
RUN pip3 install -r https://raw.githubusercontent.com/microsoft/AzureClusterlessHPC.jl/main/pyrequirements.txt

# Add julia packages and registry
RUN julia -e 'using Pkg; Pkg.update()' && \
    julia -e 'using Pkg; Pkg.Registry.add(RegistrySpec(url="https://Github.com/slimgroup/SLIMregistryJL.git"))' && \
    julia -e 'using Pkg; Pkg.add(["JOLI", "JUDI", "SegyIO", "JLD2", "Images", "PyCall", "HDF5", "PyPlot", "TimeProbeSeismic"])' && \
    julia -e 'using Pkg; ENV["AZ_BATCH_TASK_WORKING_DIR"]=""; Pkg.add(url="https://github.com/microsoft/AzureClusterlessHPC.jl")'

RUN julia -e 'using Pkg; Pkg.add(["AzStorage", "AzSessions"])'

# Configure PyCall
RUN julia -e 'using Pkg; ENV["PYTHON"]="/usr/local/bin/python3"; Pkg.build("PyCall")'

# Build and precompile packages
RUN julia -e 'using Pkg;  Pkg.build("JUDI")' && \
    julia -e 'ENV["AZ_BATCH_TASK_WORKING_DIR"]=""; using AzureClusterlessHPC' && \
    julia -e 'using PyCall, JUDI, SegyIO, JOLI, PyPlot, Images, JLD2, HDF5' && \
    julia -e 'using TimeProbeSeismic, AzSessions, AzStorage'

# Make sure julia directory is accessible
RUN chmod -R 777 /root/.julia

# Install intel hpc toolkit
RUN cd /tmp && apt-get update -y && \
    apt-get install -y software-properties-common gnupg && \
    wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB && \
    apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB && \
    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB

RUN add-apt-repository "deb https://apt.repos.intel.com/oneapi all main" && \
    apt-get update -y && \
    apt-get install -y intel-oneapi-compiler-dpcpp-cpp-and-cpp-classic

# Environment variables
ENV JULIA_DEPOT_PATH="/root/.julia" \
    PYTHONPATH="/usr/local/lib/python3.8/dist-packages"

# clean
RUN rm -rf /var/lib/apt/lists/* \
	&& apt-get clean

# Setup DEVITO env
ENV DEVITO_ARCH="nvc" 
ENV DEVITO_LANGUAGE="openacc"
ENV DEVITO_PLATFORM=nvidiaX
ENV DEVITO_LOGGING="DEBUG"

# remove older cuda version that are not needed
ADD entry_nvc.sh entry_nvc.sh
RUN chmod +x entry_nvc.sh
ENTRYPOINT ["/entry_nvc.sh"]

